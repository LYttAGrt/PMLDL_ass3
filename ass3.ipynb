{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class Field:\n",
    "    def __init__(self, tokens, pad_token, init_token=None, eos_token=None):\n",
    "        # Data: 2D strings list\n",
    "        self.data = tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.init_token = init_token\n",
    "        self.eos_token = eos_token\n",
    "        self.vocab = None\n",
    "        self.lookup = None\n",
    "        self.reverse_lookup = None\n",
    "        self.tensor = None\n",
    "        self.max_sent_length = max([len(x) for x in self.data])\n",
    "    \n",
    "    def build_vocab(self, min_freq: int):\n",
    "        # MIN_FREQ not actually needed\n",
    "        buffer = list()\n",
    "        for i in range(len(self.data)):\n",
    "            for k in range(len(self.data[i])):\n",
    "                buffer.append(self.data[i][k])\n",
    "        self.vocab = sorted(list(set(buffer)))\n",
    "        self.lookup = {value: index for index, value in enumerate(self.vocab)}\n",
    "        self.reverse_lookup = {index: value for index, value in enumerate(self.vocab)}\n",
    "    \n",
    "    def build_tensor(self, pad_to:int=None):\n",
    "        if pad_to is None:\n",
    "            pad_to = self.max_sent_length\n",
    "        self.tensor = torch.zeros(len(self.data), pad_to, dtype=int)\n",
    "        for i in range(len(self.data)):\n",
    "            for j in range(len(self.data[i])):\n",
    "                self.tensor[i, j] = self.lookup.get(self.data[i][j])\n",
    "        return self.tensor\n",
    "\n",
    "\n",
    "class Iterator:\n",
    "    def __init__(self, src: Field, trg: Field, batch_size=200, pad_to=100):\n",
    "        if len(src.data) != len(trg.data):\n",
    "            raise \"Shit!\"\n",
    "        self.src_tensor = src.build_tensor(pad_to=pad_to)\n",
    "        self.trg_tensor = trg.build_tensor(pad_to=pad_to)\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_counter = 0\n",
    "        self.batch_amount = len(src.data) // self.batch_size\n",
    "    \n",
    "    def iterate(self):\n",
    "        if self.batch_counter < self.batch_amount:\n",
    "            self.batch_counter += 1\n",
    "            return self.batch_counter, (\n",
    "                self.src_tensor[self.batch_size * (self.batch_counter-1):self.batch_counter * self.batch_size,\n",
    "                                :],\n",
    "                self.trg_tensor[self.batch_size * (self.batch_counter-1):self.batch_counter * self.batch_size,\n",
    "                                :])\n",
    "        else:\n",
    "            return -1, (None, None)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * np.sqrt(self.d_model)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int = 128, \n",
    "                 nhead: int = 8, \n",
    "                 num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, \n",
    "                 dim_feedforward: int = 128, \n",
    "                 dropout: float = 0.2,\n",
    "                 activation: str = \"relu\", \n",
    "                 source_vocab_length: int = 60000, \n",
    "                 target_vocab_length: int = 60000) -> None:\n",
    "        super(MyTransformer, self).__init__()\n",
    "        self.source_embedding = nn.Embedding(source_vocab_length, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "        self.target_embedding = nn.Embedding(target_vocab_length, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        self.out = nn.Linear(d_model, target_vocab_length)\n",
    "        self._reset_parameters()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(self, \n",
    "                src: torch.Tensor, \n",
    "                tgt: torch.Tensor, \n",
    "                src_mask: torch.Tensor = None, \n",
    "                tgt_mask: torch.Tensor = None,\n",
    "                memory_mask: torch.Tensor = None, \n",
    "                src_key_padding_mask: torch.Tensor = None,\n",
    "                tgt_key_padding_mask: torch.Tensor = None, \n",
    "                memory_key_padding_mask: torch.Tensor = None):\n",
    "        if src.size(1) != tgt.size(1):\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "        src = self.source_embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        tgt = self.target_embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data, build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 7513 in source, 5317 in target\n"
     ]
    }
   ],
   "source": [
    "en_list, ru_list = list(), list()\n",
    "len_dataset = 800 # 1000000\n",
    "\n",
    "with open('1mcorpus/corpus.en_ru.1m.en', 'r') as f:\n",
    "    en_list = [nltk.tokenize.word_tokenize(x, language='english') for x in f.readlines()[:len_dataset]]\n",
    "with open('1mcorpus/corpus.en_ru.1m.ru', 'r') as f:\n",
    "    ru_list = [nltk.tokenize.word_tokenize(x, language='russian') for x in f.readlines()[:len_dataset]]\n",
    "\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "BATCH_SIZE = 40\n",
    "\n",
    "SRC = Field(ru_list, pad_token=BLANK_WORD)\n",
    "TGT = Field(en_list, init_token=BOS_WORD, eos_token=EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "MIN_FREQ = 2\n",
    "\n",
    "SRC.build_vocab(MIN_FREQ)\n",
    "TGT.build_vocab(MIN_FREQ)\n",
    "\n",
    "source_vocab_length = len(SRC.vocab)\n",
    "target_vocab_length = len(TGT.vocab)\n",
    "print(f'We got {source_vocab_length} in source, {target_vocab_length} in target')\n",
    "\n",
    "train_iter = Iterator(SRC, TGT, BATCH_SIZE, pad_to=max([SRC.max_sent_length, TGT.max_sent_length]))\n",
    "val_iter = Iterator(SRC, TGT, 1, pad_to=max([SRC.max_sent_length, TGT.max_sent_length]))\n",
    "\n",
    "model = MyTransformer(source_vocab_length=source_vocab_length, target_vocab_length=target_vocab_length)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-6)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's go!\n",
      "Epoch [1/1] complete. Train Loss: 202.867. Val Loss: 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([202.86664611816408], [0.0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(train_iter, val_iter, model, optim, num_epochs, use_gpu=True):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    print(\"Let's go!\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        train_iter.batch_counter=0\n",
    "        val_iter.batch_counter=0\n",
    "        # Train model\n",
    "        model.train()\n",
    "        for i in range(train_iter.batch_amount):\n",
    "            cnt, batch = train_iter.iterate()\n",
    "            # print(cnt, '/', train_iter.batch_amount, batch[0].shape, batch[1].shape)\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "            # change to shape (bs , max_seq_len)\n",
    "            src = src.transpose(0, 1)\n",
    "            # change to shape (bs , max_seq_len)\n",
    "            trg = trg.transpose(0, 1)\n",
    "            \n",
    "            trg_input = trg[:, :]\n",
    "            targets = trg[:, :].contiguous().view(-1)\n",
    "            size = trg_input.size(1)\n",
    "            np_mask = torch.triu(torch.ones(size, size) == 1).transpose(0, 1)\n",
    "            np_mask = np_mask.float().masked_fill(\n",
    "                np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0.0))\n",
    "            \n",
    "            # Forward, backprop, optimizer\n",
    "            optim.zero_grad()\n",
    "            preds = model(src.transpose(0, 1), trg_input.transpose(0, 1), tgt_mask=np_mask)\n",
    "            preds = preds.transpose(0, 1).contiguous().view(-1, preds.size(-1))\n",
    "            loss = F.cross_entropy(preds, targets, ignore_index=0, reduction='sum')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.item() / BATCH_SIZE\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(val_iter.batch_amount):\n",
    "                cnt, batch = val_iter.iterate()\n",
    "                src = batch[0]\n",
    "                trg = batch[1]\n",
    "                # change to shape (bs , max_seq_len)\n",
    "                src = src.transpose(0, 1)\n",
    "                # change to shape (bs , max_seq_len+1) , Since right shifted\n",
    "                trg = trg.transpose(0, 1)\n",
    "                trg_input = trg[:, :-1]\n",
    "                targets = trg[:, 1:].contiguous().view(-1)\n",
    "                size = trg_input.size(1)\n",
    "                # print(size)\n",
    "                np_mask = torch.triu(torch.ones(size, size) == 1).transpose(0, 1)\n",
    "                np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(\n",
    "                    np_mask == 1, float(0.0))\n",
    "                \n",
    "                preds = model(src.transpose(0, 1), trg_input.transpose(0, 1), tgt_mask=np_mask)\n",
    "                preds = preds.transpose(0, 1).contiguous().view(-1, preds.size(-1))\n",
    "                loss = F.cross_entropy(preds, targets, ignore_index=0, reduction='sum')\n",
    "                valid_loss += loss.item() / 1\n",
    "\n",
    "        # Log after each epoch\n",
    "        print(\n",
    "            f'''Epoch [{epoch + 1}/{num_epochs}] complete. Train Loss: {train_loss / train_iter.batch_amount:.3f}. Val Loss: {valid_loss / val_iter.batch_amount:.3f}''')\n",
    "\n",
    "        train_losses.append(train_loss / train_iter.batch_amount)\n",
    "        valid_losses.append(valid_loss / val_iter.batch_amount)\n",
    "\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "\n",
    "train(train_iter=train_iter, val_iter=val_iter, model=model, optim=optim, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\n",
      "tensor([4907])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "sentence = [SRC.reverse_lookup.get(i) for i in range(1000,1010)]\n",
    "indexed = []\n",
    "for tok in sentence:\n",
    "    if SRC.lookup.get(tok) is not None:\n",
    "        indexed.append(SRC.lookup.get(tok))\n",
    "    else:\n",
    "        indexed.append(0)\n",
    "\n",
    "sentence = torch.LongTensor([indexed])\n",
    "print(indexed)\n",
    "trg_init_tok = 0\n",
    "trg = torch.LongTensor([[trg_init_tok]])\n",
    "translated_sentence = \"\"\n",
    "for i in range(len(sentence)):\n",
    "    size = trg.size(0)\n",
    "    np_mask = torch.triu(torch.ones(size, size)==1).transpose(0,1)\n",
    "    np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0.0))\n",
    "    pred = model(sentence.transpose(0,1), trg, tgt_mask = np_mask).argmax(dim=2)[-1]\n",
    "    print(pred)\n",
    "    add_word = TGT.reverse_lookup.get(pred.detach().numpy()[0])\n",
    "    translated_sentence+=str(add_word)\n",
    "    if add_word==EOS_WORD:\n",
    "        break\n",
    "    trg = torch.cat((trg,torch.LongTensor([[pred[-1]]])))\n",
    "# TGT.reverse_lookup.get(11960)\n",
    "translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "wf = open('answers.txt', 'w')\n",
    "\n",
    "with open('test.ru.txt', 'r') as rf:\n",
    "    results = []\n",
    "    for line in rf.readlines():\n",
    "        sentence = nltk.tokenize.word_tokenize(line, language='russian')\n",
    "        indexed = []\n",
    "        for tok in sentence:\n",
    "            if SRC.lookup.get(tok) is not None:\n",
    "                indexed.append(SRC.lookup.get(tok))\n",
    "            else:\n",
    "                indexed.append(0)\n",
    "        sentence = torch.LongTensor([indexed])\n",
    "        trg_init_tok = 0\n",
    "        trg = torch.LongTensor([[trg_init_tok]])\n",
    "        translated_sentence = \"\"\n",
    "        for i in range(len(sentence)):\n",
    "            size = trg.size(0)\n",
    "            np_mask = torch.triu(torch.ones(size, size)==1).transpose(0,1)\n",
    "            np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0.0))\n",
    "            pred = model(sentence.transpose(0,1), trg, tgt_mask = np_mask).argmax(dim=2)[-1]\n",
    "            add_word = TGT.reverse_lookup.get(pred.detach().numpy()[0])\n",
    "            translated_sentence+=\" \"+str(add_word)\n",
    "            if add_word==EOS_WORD:\n",
    "                break\n",
    "            trg = torch.cat((trg,torch.LongTensor([[pred[-1]]])))\n",
    "        results.append(translated_sentence)\n",
    "    line = '\\n'.join(results)\n",
    "    print(len(results))\n",
    "    wf.write(line)\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
